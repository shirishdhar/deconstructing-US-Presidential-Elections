{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Pro-Trump or Pro-Hillary Tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics Feature\n",
    "topics taken from https://github.com/WiMLDS/election-data-hackathon/blob/master/candidate-tweets-oct-2016/data/ReportTwitterAnalysisofPresidentialCandidates.pdf\n",
    "\n",
    "vectorized feature; 1 if the a word in the topic is in the tweet else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# given a topic for each tweet if a word in topic X exists in the tweet then the feature corresponding is 1, else 0\n",
    "def topicFeaturesExtract(tweet):\n",
    "    tweetWords = set(tweet.split())\n",
    "    topics = ['email', 'russia', 'race', 'immigration', 'trust', 'sex', 'female', 'male']\n",
    "    output = [0] * len(topics)\n",
    "    \n",
    "    wordLists = {}\n",
    "    wordLists['email']  = ['emails', 'email', 'crookedhillary', 'prison', 'crooked']\n",
    "    wordLists['russia'] =  ['putin', 'russia', 'crimea', 'vladimir', 'ukraine', 'russian']\n",
    "    wordLists['race']   = ['white', 'black', 'racist', 'race']\n",
    "    wordLists['immigration'] = ['borders', 'border', 'wall', 'mexico', 'illegal', 'immigrants', 'immigration', \n",
    "                               'trafficking']\n",
    "    wordLists['trust'] = ['factcheck', 'fact', 'factchecking', 'bigleaguetruth', 'trust', 'politifact', \n",
    "                          'trustworthiness', 'lies', 'lie', 'truth', 'liar']\n",
    "    wordLists['sex'] = ['sex', 'sexual', 'assault', 'rape', 'rapist', 'transgression', 'transgressions']\n",
    "    \n",
    "    wordLists['female'] = ['she', 'her', 'she\\'s', 'herself']\n",
    "    wordLists['male'] = ['he', 'his', 'he\\'s', 'himself', 'him']\n",
    "    \n",
    "    for idx, val in enumerate(topics):\n",
    "        if len(tweetWords.intersection(wordLists[val])) > 0:\n",
    "            output[idx] = 1\n",
    "        else:\n",
    "            output[idx] = 0     \n",
    "    \n",
    "    return output\n",
    "\n",
    "# Given a list of tweets return list of lists each sublist is a topic\n",
    "def topicFeatures(tweetList):\n",
    "    totalFeatures = [[], [], [], [], [], [], [], []]\n",
    "    for tweet in tweetList:\n",
    "        feats = topicFeaturesExtract(tweet)\n",
    "       \n",
    "        for idx, val in enumerate(feats):\n",
    "            totalFeatures[idx].append(val)\n",
    "        \n",
    "    return totalFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name Occurrence Feature\n",
    "Three features:\n",
    "1. If the words (trump, pence, donald) exists then 1 else 0\n",
    "2. If the words (hillary, kaine, clinton, tim) exists then 1 else 0\n",
    "3. If (bill) exists then 1 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def trump_occurence(tweet):\n",
    "    if (('trump' in nltk.word_tokenize(tweet.lower())) or \n",
    "        ('pence' in nltk.word_tokenize(tweet.lower())) or ('donald' in nltk.word_tokenize(tweet.lower()))):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def hillary_occurence(tweet):\n",
    "    if (('hillary' in nltk.word_tokenize(tweet.lower())) \n",
    "        or ('kaine' in nltk.word_tokenize(tweet.lower())) \n",
    "        or ('clinton' in nltk.word_tokenize(tweet.lower())) \n",
    "        or ('tim' in nltk.word_tokenize(tweet.lower()))):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def bill_occurence(tweet):\n",
    "    if 'bill' in nltk.word_tokenize(tweet.lower()):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Feature\n",
    "These are four features two for the hillary camp (hillary, clinton, kaine, tim) and two for the trump camp (trump, donald, pence, mike). Given a list of negative words, the first two feature computes the minimum distance between any word in the camp and a negative word if one exists in the tweet. If no negative words exist or if no name from the camp exists then return 20. 20 is selected because it is highly unlikely a tweet will have 20 words. The third and fourth features are the same but with a list of positive words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate a negative word list and positive word list using the entire data set. \n",
    "# Take every unigram in the tweets and apply vader on each  unigram, \n",
    "# the word list is all unigrams with a negative vader score = 1\n",
    "\n",
    "df1 = pd.DataFrame.from_csv('Improved Hillary Tweets.csv')\n",
    "df2 = pd.DataFrame.from_csv('Improved Trump Tweets.csv')\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "k1 = [tweet.split() for tweet in df1['clean_tweets']]\n",
    "k1 = [item for sublist in k1 for item in sublist]\n",
    "k1 = [(word.lower(), sid.polarity_scores(word)['neg'], sid.polarity_scores(word)['pos']) for word in k1]\n",
    "\n",
    "k2 = [tweet.split() for tweet in df2['clean_tweets']]\n",
    "k2 = [item for sublist in k2 for item in sublist]\n",
    "k2 = [(word.lower(), sid.polarity_scores(word)['neg'],sid.polarity_scores(word)['pos']) for word in k2]\n",
    "\n",
    "k = set(k1 + k2)\n",
    "\n",
    "neg_word_list = sorted(k, key=lambda x: x[1], reverse=True)\n",
    "neg_word_list = [t[0] for t in k if t[1] == 1.0] \n",
    "\n",
    "# add the words not hasnt cant wont didnt in the list\n",
    "neg_word_list.append('not')\n",
    "neg_word_list.append(\"hasnt\")\n",
    "neg_word_list.append(\"cant\")\n",
    "neg_word_list.append(\"wont\")\n",
    "neg_word_list.append(\"didnt\")\n",
    "neg_word_list.append(\"doesnt\")\n",
    "neg_word_list.append(\"n\\'t\")\n",
    "\n",
    "# generate a positive word list as well\n",
    "pos_word_list = sorted(k, key=lambda x: x[2], reverse=True)\n",
    "pos_word_list = [t[0] for t in k if t[2] == 1.0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_word_list = pickle.load(open(\"neg_word_list.pkl\",\"rb\"))\n",
    "pos_word_list = pickle.load(open(\"pos_word_list.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dist_hillary(tweet, input_wordList):\n",
    "    wordlist = nltk.word_tokenize(tweet.lower())\n",
    "    distances = []\n",
    "    \n",
    "    index = []\n",
    "    for name in ['hillary','kaine','tim','clinton']:\n",
    "        try:\n",
    "            index.append(wordlist.index(name))\n",
    "        except:\n",
    "            a=1\n",
    "    \n",
    "    if len(index) == 0:\n",
    "        return 20\n",
    "    \n",
    "    for item in wordlist:\n",
    "        if item in input_wordList:\n",
    "            negative_index = wordlist.index(item)\n",
    "            for idx in index:\n",
    "                distances.append(abs(negative_index - idx))\n",
    "    if len(distances)!=0:\n",
    "        return min(distances)\n",
    "    else:\n",
    "        return 20\n",
    "            \n",
    "def dist_trump(tweet, input_wordList):\n",
    "    wordlist = nltk.word_tokenize(tweet.lower())\n",
    "    distances = []\n",
    "    index = []\n",
    "    for name in ['donald','trump','mike','pence']:\n",
    "        try:\n",
    "            index.append(wordlist.index(name))\n",
    "        except:\n",
    "            a=1\n",
    "    \n",
    "    if len(index) == 0:\n",
    "        return 20\n",
    "    for item in wordlist:\n",
    "        if item in input_wordList:\n",
    "            negative_index = wordlist.index(item)\n",
    "            for idx in index:\n",
    "                distances.append(abs(negative_index - idx))\n",
    "    if len(distances)!=0:\n",
    "        return min(distances)\n",
    "    else:\n",
    "        return 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-gram sentiment feature\n",
    "Pick n-gram around the important names and then output the sum of the compound scores from vader for those compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def tweet_processing(tweet_list):\n",
    "    processed_list = []\n",
    "    for item in tweet_list:\n",
    "        item = item.lower().replace('hillary clinton', 'clinton')\n",
    "        item = item.lower().replace('hrc', 'clinton')\n",
    "        item = item.lower().replace('bill clinton', 'bill')\n",
    "        item = item.lower().replace('donald trump', 'trump')\n",
    "        item = item.lower().replace('tim kaine', 'kaine')\n",
    "        item = item.lower().replace('mike pence', 'pence')\n",
    "        item = item.lower().replace('@hillaryclinton', 'clinton')\n",
    "        item = item.lower().replace('@timkaine', 'kaine')\n",
    "        item = item.lower().replace('@realDonaldTrump', 'trump')\n",
    "        item = item.lower().replace('@mike_pence', 'pence')\n",
    "        processed_list.append(item)\n",
    "    return processed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def tokenize_text(corpus):\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus) # Split text into sentences\n",
    "    \n",
    "    output = []\n",
    "    for sent in raw_sents:\n",
    "        output = output + nltk.word_tokenize(sent)\n",
    "        \n",
    "    return output\n",
    "\n",
    "def create_sentences(data_frame):\n",
    "    mylist = []\n",
    "    for item in data_frame:\n",
    "        mylist.append(tokenize_text(item))\n",
    "    \n",
    "    return mylist\n",
    "\n",
    "def ngram_features_sum(tweet, n=4):\n",
    "    hillary_noun_list = ['hillary','clinton','kaine', 'tim']\n",
    "    trump_noun_list   = ['trump', 'donald', 'mike', 'pence']\n",
    "    features = [[],[]]\n",
    "    score_list_hillary = 0\n",
    "    score_list_trump = 0\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "#     result_set    = create_sentences(tweet)\n",
    "    sentence_list = create_sentences(tweet)\n",
    "#     sentence_list = flatten_sentences(result_set)\n",
    "#     print(len(sentence_list))\n",
    "    for tokens in sentence_list:\n",
    "#         print(tokens)\n",
    "        sum_trump = []\n",
    "        sum_hillary = []\n",
    "        for item in tokens:\n",
    "            index = tokens.index(item)\n",
    "            if index-n < 0:\n",
    "                extraEnd = n-index\n",
    "            else:\n",
    "                extraEnd = 0\n",
    "\n",
    "            if index+n > (len(tokens)-1):\n",
    "                extraStart = index+n - (len(tokens)-1)\n",
    "            else:\n",
    "                extraStart = 0\n",
    "\n",
    "            start = max(0, index-n-extraStart)\n",
    "            end   = min(len(tokens)-1, index+n+extraEnd) \n",
    "            n_gram = tokens[start:end+1]\n",
    "            flat_tweet = ' '.join(n_gram)\n",
    "            ss = sid.polarity_scores(flat_tweet)\n",
    "            if item in trump_noun_list:\n",
    "                sum_trump.append(ss['compound'])\n",
    "            if item in hillary_noun_list:\n",
    "                sum_hillary.append(ss['compound'])\n",
    "                \n",
    "        if(len(sum_trump) == 0):\n",
    "            score_list_trump = 0\n",
    "        else:\n",
    "            score_list_trump = sum(sum_trump)\n",
    "        \n",
    "        if(len(sum_hillary) == 0):\n",
    "            score_list_hillary = 0\n",
    "        else:\n",
    "            score_list_hillary = sum(sum_hillary)\n",
    "            \n",
    "        #score_list_trump = sum(sum_trump) / float(len(sum_trump))\n",
    "        #score_list_hillary = sum(sum_hillary) / float(len(sum_hillary))\n",
    "        features[0].append(score_list_trump)\n",
    "        features[1].append(score_list_hillary)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Features\n",
    "Use the above functions to generate the feature set, the input csv contains two columns\n",
    "1. preprocessed tweet\n",
    "3. our tag as to whether the tweet is pro-trump or pro-hillary (only for the training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df is a pandas dataframe with Tweet column\n",
    "def extractFeatures(df):\n",
    "    df['Tweet'] = tweet_processing(df['Tweet'].tolist())\n",
    "    \n",
    "    # distance feature\n",
    "    trump_distance_negative_word   = []\n",
    "    hillary_distance_negative_word = []\n",
    "    trump_distance_positive_word   = []\n",
    "    hillary_distance_positive_word = []\n",
    "    trump_occured                  = []\n",
    "    hillary_occured                = []\n",
    "    bill_occured                   = []\n",
    "\n",
    "    for tweet in df['Tweet']:\n",
    "        trump_distance_negative_word.append(dist_trump(tweet, neg_word_list))\n",
    "        hillary_distance_negative_word.append(dist_hillary(tweet, neg_word_list))\n",
    "        trump_distance_positive_word.append(dist_trump(tweet, pos_word_list))\n",
    "        hillary_distance_positive_word.append(dist_hillary(tweet, pos_word_list))\n",
    "        trump_occured.append(trump_occurence(tweet))\n",
    "        hillary_occured.append(hillary_occurence(tweet))\n",
    "        bill_occured.append(bill_occurence(tweet))\n",
    "    \n",
    "    # topic and adjective features\n",
    "    topicFeats = topicFeatures(df['Tweet'])\n",
    "    \n",
    "    # ngram vader features\n",
    "    value  = ngram_features_sum(df['Tweet'])\n",
    "\n",
    "    # compound vader score for entire tweet\n",
    "    sid    = SentimentIntensityAnalyzer()\n",
    "    scores = [sid.polarity_scores(tweet)['compound'] for tweet in df['Tweet']]\n",
    "    \n",
    "    # combine the features into an output\n",
    "    allFeatures = pd.DataFrame()\n",
    "\n",
    "    allFeatures['trump_distance_negative_word']   = trump_distance_negative_word\n",
    "    allFeatures['hillary_distance_negative_word'] = hillary_distance_negative_word\n",
    "    allFeatures['trump_distance_positive_word']   = trump_distance_positive_word\n",
    "    allFeatures['hillary_distance_positive_word'] = hillary_distance_positive_word\n",
    "    allFeatures['trump_occured']                  = trump_occured\n",
    "    allFeatures['hillary_occured']                = hillary_occured\n",
    "    allFeatures['bill_occured']                   = bill_occured\n",
    "\n",
    "    topics = ['email', 'russia', 'race', 'immigration', 'trust', 'sex', 'female', 'male']\n",
    "    for j in range(0,8):\n",
    "        allFeatures[topics[j]] = topicFeats[j]\n",
    "    \n",
    "    allFeatures['trump_ngram_vader']   = value[0]\n",
    "    allFeatures['hillary_ngram_vader'] = value[1]\n",
    "    \n",
    "    allFeatures['Score'] = scores\n",
    "    \n",
    "    return allFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# x = pd.DataFrame.from_csv(\"tagged_tweets_harman - Sheet1.csv\")\n",
    "y = pd.DataFrame.from_csv(\"pro_trump - Sheet1.csv\")\n",
    "z = pd.DataFrame.from_csv(\"Final_csv.csv\")\n",
    "x = pd.DataFrame.from_csv(\"allTaggedCANY.csv\")\n",
    "w = pd.DataFrame.from_csv(\"protrumptweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# some tweets are neither pro trump nor pro hillary so there is no tag and hence drop them\n",
    "x.dropna(axis=0, inplace=True)\n",
    "x.index = range(0,len(x))\n",
    "\n",
    "y.dropna(axis=0, inplace=True)\n",
    "y.index = range(0,len(y))\n",
    "\n",
    "z.dropna(axis=0, inplace=True)\n",
    "z.index = range(0,len(z))\n",
    "\n",
    "w.dropna(axis=0, inplace=True)\n",
    "w.index = range(0,len(w))\n",
    "\n",
    "tot = pd.concat([w,x,y,z])\n",
    "tot = tot.drop_duplicates(subset='Tweet')\n",
    "tot.index = range(0,len(tot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allFeatures           = extractFeatures(tot)\n",
    "\n",
    "allFeatures['Winner'] = tot['Winner']\n",
    "allFeatures.to_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allFeatures = pd.DataFrame.from_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "811\n",
      "296.0\n"
     ]
    }
   ],
   "source": [
    "# take equal number of trump and hillary tweets\n",
    "# allFeatures = allFeatures[allFeatures['Score'] != 0]\n",
    "print(len(allFeatures))\n",
    "print(sum(allFeatures['Winner']))\n",
    "\n",
    "numTrump      = int(sum(tot['Winner']))\n",
    "TrumpTweets   = allFeatures[allFeatures['Winner'] == 1]\n",
    "HillaryTweets = allFeatures[allFeatures['Winner'] == 0]\n",
    "HillaryTweets = HillaryTweets.sample(numTrump)\n",
    "\n",
    "HillaryTweets.index = range(0, numTrump)\n",
    "\n",
    "allFeats = pd.concat([TrumpTweets, HillaryTweets])\n",
    "allFeats = allFeats.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# read in the training data (which is just allFeatures from above but doing this for modularity)\n",
    "\n",
    "# data    = pd.DataFrame.from_csv('train.csv')\n",
    "data    = allFeats\n",
    "datanew = data\n",
    "datanew = datanew.drop('Winner',1)\n",
    "\n",
    "trainData, devData, trainTarget, devTarget = train_test_split(datanew, data['Winner'], train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a MinMaxScaler to normalize the features and going to use a mlp nerual net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scaler    = MinMaxScaler()\n",
    "learner   = MLPClassifier(solver='adam', hidden_layer_sizes=(100,), max_iter=500)\n",
    "voter     = BaggingClassifier(base_estimator=learner, n_estimators=5)\n",
    "pipeline  = Pipeline([('scaler', scaler), ('learner', voter)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('learner', BaggingClassifier(base_estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='cons..._estimators=5, n_jobs=1, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(trainData,trainTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76470588235294112"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(devData, devTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HONESTCLASSIFIER.pkl']"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(pipeline,\"HONESTCLASSIFIER.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipeline.predict(devData)\n",
    "\n",
    "final = pd.DataFrame()\n",
    "final['predicted'] = pipeline.predict(devData)\n",
    "final.index = devTarget.index\n",
    "final['true']      = devTarget\n",
    "final['tweet']     = x['Tweet'][devTarget.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the classifier onto other states and get electoral map results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "states = {\n",
    "        'AK': 'Alaska',\n",
    "        'AL': 'Alabama',\n",
    "        'AR': 'Arkansas',\n",
    "        'AS': 'American Samoa',\n",
    "        'AZ': 'Arizona',\n",
    "        'CA': 'California',\n",
    "        'CO': 'Colorado',\n",
    "        'CT': 'Connecticut',\n",
    "        'DC': 'District of Columbia',\n",
    "        'DE': 'Delaware',\n",
    "        'FL': 'Florida',\n",
    "        'GA': 'Georgia',\n",
    "        'GU': 'Guam',\n",
    "        'HI': 'Hawaii',\n",
    "        'IA': 'Iowa',\n",
    "        'ID': 'Idaho',\n",
    "        'IL': 'Illinois',\n",
    "        'IN': 'Indiana',\n",
    "        'KS': 'Kansas',\n",
    "        'KY': 'Kentucky',\n",
    "        'LA': 'Louisiana',\n",
    "        'MA': 'Massachusetts',\n",
    "        'MD': 'Maryland',\n",
    "        'ME': 'Maine',\n",
    "        'MI': 'Michigan',\n",
    "        'MN': 'Minnesota',\n",
    "        'MO': 'Missouri',\n",
    "        'MP': 'Northern Mariana Islands',\n",
    "        'MS': 'Mississippi',\n",
    "        'MT': 'Montana',\n",
    "        'NA': 'National',\n",
    "        'NC': 'North Carolina',\n",
    "        'ND': 'North Dakota',\n",
    "        'NE': 'Nebraska',\n",
    "        'NH': 'New Hampshire',\n",
    "        'NJ': 'New Jersey',\n",
    "        'NM': 'New Mexico',\n",
    "        'NV': 'Nevada',\n",
    "        'NY': 'New York',\n",
    "        'OH': 'Ohio',\n",
    "        'OK': 'Oklahoma',\n",
    "        'OR': 'Oregon',\n",
    "        'PA': 'Pennsylvania',\n",
    "        'PR': 'Puerto Rico',\n",
    "        'RI': 'Rhode Island',\n",
    "        'SC': 'South Carolina',\n",
    "        'SD': 'South Dakota',\n",
    "        'TN': 'Tennessee',\n",
    "        'TX': 'Texas',\n",
    "        'UT': 'Utah',\n",
    "        'VA': 'Virginia',\n",
    "        'VI': 'Virgin Islands',\n",
    "        'VT': 'Vermont',\n",
    "        'WA': 'Washington',\n",
    "        'WI': 'Wisconsin',\n",
    "        'WV': 'West Virginia',\n",
    "        'WY': 'Wyoming'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# generate total tweets that has state information\n",
    "h = pd.DataFrame.from_csv(\"Improved Hillary Tweets.csv\")\n",
    "t = pd.DataFrame.from_csv(\"Improved Trump Tweets.csv\")\n",
    "\n",
    "tweets = h['clean_tweets'].tolist() + t['clean_tweets'].tolist()\n",
    "locs   = h['state_abbs'].tolist() + t['state_abbs'].tolist()\n",
    "\n",
    "totalFrame = pd.DataFrame()\n",
    "totalFrame['Tweet'] = tweets\n",
    "totalFrame['loc']   = locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "truelocs = list(states.keys())\n",
    "for extra in ['AS', 'GU', 'MP', 'NA', 'VI', 'PR']:\n",
    "    truelocs.remove(extra)\n",
    "\n",
    "totalFrame = totalFrame[totalFrame['loc'].isin(truelocs)]\n",
    "totalFrame.sort_values(by='loc', inplace=True)\n",
    "totalFrame.index = range(0,len(totalFrame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "totalFrameFeats = extractFeatures(totalFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def getStatePredictions(df):\n",
    "    locs = list(states.keys())\n",
    "    for extra in ['AS', 'GU', 'MP', 'NA', 'VI', 'PR']:\n",
    "        locs.remove(extra)\n",
    "    \n",
    "    locs.sort()\n",
    "    result = []\n",
    "\n",
    "    for j in range(0,51):\n",
    "        tmp = sum(df[df['loc'] == locs[j]]['mlanswer'])\n",
    "        \n",
    "        if tmp > len(df[df['loc'] == locs[j]]['mlanswer'])/2:\n",
    "            result.append(\"Trump\")\n",
    "        else:\n",
    "            result.append(\"Hillary\")\n",
    "            \n",
    "    # write out the predicted results\n",
    "    resultsDf = pd.DataFrame()\n",
    "    \n",
    "    # write also real results and sentiment results\n",
    "    realResult = pd.DataFrame.from_csv(\"electionresult.csv\")\n",
    "    sentiresl  = pd.DataFrame.from_csv(\"predictedResults.csv\")\n",
    "    stateAvgs  = pd.DataFrame.from_csv(\"stateAvgs.csv\")\n",
    "\n",
    "    resultsDf[\"state\"]           = locs\n",
    "    \n",
    "    resultsDf['Hillary Tweets']  = stateAvgs['H Num']\n",
    "    resultsDf['Trump Tweets']    = stateAvgs['T Num']\n",
    "    resultsDf['Sentiment Result'] = sentiresl[\"Prediction\"]\n",
    "\n",
    "    resultsDf[\"ML Prediction\"]   = result\n",
    "    resultsDf['Real Result']     = realResult[\"Winner\"].values\n",
    "    \n",
    "    resultsDf.to_csv(\"predictedResultsML.csv\")\n",
    "   \n",
    "    # states in which tweets where taken for training the neural nets\n",
    "    trainingStates = ['CA', 'NY', 'KY']\n",
    "    elimStates     = trainingStates\n",
    "    for s in resultsDf[\"state\"]:\n",
    "        if (resultsDf[resultsDf[\"state\"]==s][\"Hillary Tweets\"].tolist()[0] + \n",
    "            resultsDf[resultsDf[\"state\"]==s][\"Trump Tweets\"].tolist()[0]) < 150:\n",
    "            elimStates.append(s)\n",
    "    \n",
    "    resultsDf = resultsDf[resultsDf['state'].isin(elimStates) == False]\n",
    "    numStates = len(resultsDf)\n",
    "    \n",
    "    resultsDf.index = range(0,numStates)\n",
    "    # print an accuracy score\n",
    "    print(\"Accuracy Score: \" + str(sum([ x==y for (x,y) in \n",
    "                                    zip(resultsDf[\"ML Prediction\"].values, \n",
    "                                        resultsDf[\"Real Result\"].values.tolist())])))\n",
    "    \n",
    "    return resultsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'totalFrameFeats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-e9b7692cb580>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# pipeline.fit(datanew, data['Winner'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotalFrameFeats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtotalFrame\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mlanswer'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'totalFrameFeats' is not defined"
     ]
    }
   ],
   "source": [
    "# pipeline.fit(datanew, data['Winner'])\n",
    "\n",
    "result = pipeline.predict(totalFrameFeats)\n",
    "\n",
    "totalFrame['mlanswer'] = result\n",
    "\n",
    "resultsDf = getStatePredictions(totalFrame)\n",
    "\n",
    "resultsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resultsDf.to_csv(\"goodResult.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "pipeline = joblib.load(\"HONESTCLASSIFIER.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43    Can't wait for Hillary supporters who have a p...\n",
      "Name: Tweet, dtype: object\n",
      "Hillary\n"
     ]
    }
   ],
   "source": [
    "hillary = pd.DataFrame.from_csv(\"HillaryWithSentiments.csv\")\n",
    "trump = pd.DataFrame.from_csv(\"TrumpWithSentiments.csv\")\n",
    "temp = pd.DataFrame()\n",
    "\n",
    "x = 43\n",
    "\n",
    "# temp[\"Tweet\"] = hillary[hillary.index.isin([x])][\"clean_tweets\"]\n",
    "temp[\"Tweet\"] = trump[trump.index.isin([x])][\"clean_tweets\"]\n",
    "\n",
    "print(temp[\"Tweet\"])\n",
    "readableOutput = {1: 'Trump', 0: 'Hillary'}\n",
    "\n",
    "print(readableOutput[pipeline.predict(extractFeatures(temp))[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump\n"
     ]
    }
   ],
   "source": [
    "# ENTER YOUR TWEET HERE\n",
    "yourTweet = \"\"\n",
    "temp = pd.DataFrame()\n",
    "temp[\"Tweet\"] = [yourTweet]\n",
    "\n",
    "readableOutput = {1: 'Trump', 0: 'Hillary'}\n",
    "\n",
    "print(readableOutput[pipeline.predict(extractFeatures(temp))[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trump_distance_negative_word</th>\n",
       "      <th>hillary_distance_negative_word</th>\n",
       "      <th>trump_distance_positive_word</th>\n",
       "      <th>hillary_distance_positive_word</th>\n",
       "      <th>trump_occured</th>\n",
       "      <th>hillary_occured</th>\n",
       "      <th>bill_occured</th>\n",
       "      <th>email</th>\n",
       "      <th>russia</th>\n",
       "      <th>race</th>\n",
       "      <th>immigration</th>\n",
       "      <th>trust</th>\n",
       "      <th>sex</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>trump_ngram_vader</th>\n",
       "      <th>hillary_ngram_vader</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.4023</td>\n",
       "      <td>-0.4023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trump_distance_negative_word  hillary_distance_negative_word  \\\n",
       "0                            20                              20   \n",
       "\n",
       "   trump_distance_positive_word  hillary_distance_positive_word  \\\n",
       "0                            20                               7   \n",
       "\n",
       "   trump_occured  hillary_occured  bill_occured  email  russia  race  \\\n",
       "0              0                1             0      0       0     0   \n",
       "\n",
       "   immigration  trust  sex  female  male  trump_ngram_vader  \\\n",
       "0            0      1    0       1     0                  0   \n",
       "\n",
       "   hillary_ngram_vader   Score  \n",
       "0              -0.4023 -0.4023  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractFeatures(temp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
